---
title: In Situ Acceleration-Speed Profile
author: 'Lasse Ish√∏i'
date: "`r Sys.Date()`"
categories: []
tags: []
subtitle: "Automisation using using the CatapultR package"
summary: "A simple how-to guide using R Studio"
authors: []
lastmod: "`r Sys.Date()`"
featured: yes
image: 
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

### Introduction

The In Situ Acceleration-Speed (AS) Profile has gained increasing popularity since introduced by JB Morin and team in 2021. Briefly described, the AS profile is in-game mapping of the maximal acceleration and speed capabilities based on instantaneous acceleration and speed data from GPS or equivalent tracking devices from multiple training sessions/games. For an in depth overview of the concept, a good place to start is the __[blogpost](https://jbmorin.net/2021/07/29/the-in-situ-sprint-profile-for-team-sports-testing-players-without-testing-them/)__  or __[Presentation](https://www.youtube.com/watch?v=lZTDcP8hacE&list=PLtdwe9oxCz-mBOVFfqnPs25Ru-2NDG6mz&index=2&t=1224s)__ by JB Morin and/or the following scientific papers: 

* __[Individual acceleration-speed profile in-situ: A proof of concept in professional football players](https://pubmed.ncbi.nlm.nih.gov/34023754/)__
* __[Reliability of individual acceleration-speed profile in-situ in elite youth soccer players](https://pubmed.ncbi.nlm.nih.gov/37163953/)__
* __[Minimal Number of Events Required for Acceleration-Speed Profiling in Elite Women's Soccer](https://pubmed.ncbi.nlm.nih.gov/37741637/)__

### The simple but tedious way of calculating the AS Profile

Only a few GPS providers allow for the calculation of the AS profile in the cloud using proprietary software, so being able to calculate the AS profile yourself may be valuable. Luckily, this can be simply done by using the __[web application](https://libm-lab.univ-st-etienne.fr/as-profile/#/home)__ by Yann Le Mat - all it takes is to upload the raw GPS data as a .csv file. 

Although this seems easy enough, several training sessions are needed for a valid and reliable estimation of the AS profile (see linked papers above), thus calculating the AS profile for a whole squad can easily mean that 100-200 .csv files need to be downloaded (typically 1 file per player per session with 30.000-100.000 rows depending on the session duration). This quickly becomes very tedious work!

### The easy way using the CatapultR Package and R Studio

Another, and much easier, method (at least if working with Catapult GPS data) is to leverage the power of R and the __[catapultR package](http://catapultr.catapultsports.com/)__ which serves as a wrapper for the Catapult API. This essentially means that no manual downloading of .csv files are needed. 

To get started in R Studio, I load the following packages:
```{r load libraries, echo=TRUE, message=FALSE}

library(catapultR)
library(tidyverse)
library(lubridate)

```

and set a few placeholder variables and the API connection:
```{r placeholder, echo=TRUE, eval=FALSE}

#Set player name, and date range for the data of interest
playerName <- name
startDate <- startdate
endDate <- enddate


#Get API Access
sToken <- stoken
sRegion <- sregion


#Setup API connection
token <- ofCloudCreateToken(sToken = sToken,  sRegion = sRegion)

```

The next code chunks uses several functions from the CatapultR package to retrieve the raw 10HZ sensor data via the API connection. I use the three parameters (defined above) __playerName__, __startDate__, and __endDate__ to specify the data of interest.

I first extract a list of athletes, format the start and end dates, and extract a list of activities in the specified period:

```{r get data from API, echo=TRUE, eval=FALSE}

# Get a list of athletes connected to the Token user
athletes <- ofCloudGetAthletes(token)

# Use the playerName to identify the associated athlete id
athletes$player_name <- paste(athletes$first_name, athletes$last_name)
athleteID <- athletes$id[athletes$player_name==playerName]

# Get a list activities from the specified period
from <- as.integer(as.POSIXct(as.Date(startDate, format="%d-%m-%Y")))
to <- as.integer(as.POSIXct(as.Date(endDate, format="%d-%m-%Y")))
activities <- ofCloudGetActivities(token, from = from, to = to)

```

I then create a for loop to loop over the list of activities, and extract activities where the player of interest is present:

```{r loop, echo=TRUE, eval=FALSE}

# Loop over the activities for the specific athlete to get the 10Hz raw data
# Create output variable as a list using the vector function with length equal to number of activities
rawData <- vector("list", nrow(activities))


# For loop using the ofCloudGetActivitySensorData function which takes the athleteID and an activity as arguments. 
#"Try" function suppresses an error in the for loop if the athlete is not part of an activity.
for (i in seq_along(activities)) {try(
rawData[[i]] <- ofCloudGetActivitySensorData(
  token,
  athlete_id = athleteID,
  activity_id = activities$id[[i]],
  parameters = c("ts", 
                 "cs", 
                 "lat", 
                 "long", 
                 "xy", 
                 "o", 
                 "v", 
                 "rv", 
                 "a", 
                 "hr", 
                 "pl", 
                 "sl", 
                 "mp", 
                 "pq", 
                 "ref", 
                 "hdop")), 
  silent = TRUE)
}

```

Now that I have the raw 10Hz sensor data for each activity extracted and in a list, I use the "map" (that applies a function on a list) and "unnest" functions to unnest to gps variables for each activity:

```{r map, echo=TRUE, eval=FALSE}

# Map function to unnest the data (gps variables) column across the list. 
# "Possibly" function secures that the unnest function does not stop due to error

rawData_list <- rawData %>%
                            map(possibly(~unnest(data = ., cols = "data"), NULL)) %>%
# Delete empty lists - these are activities where the player of interest has not participated
                            compact()

```

And finally, I rename the columns and create a few additional time columns (timestamp, seconds, elapsed_time):

```{r time col, echo=TRUE, eval=FALSE}

# Mutate a time column using lubridate based on ts (epoch), and rename columns
rawData_list <-  rawData_list %>% map(~mutate(.data = ., timestamp = as_datetime(.$ts),
                                                          seconds = round(cs/100, 1),
                                                          elapsed_time = (row_number()-1)/10)) %>%
                                  map(~rename(.data = ., latitude = lat,
                                              longitude = long,
                                              position_x = x,
                                              position_y = y,
                                              velocity = v,
                                              velocity_raw = rv,
                                              acceleration = a,
                                              total_distance = o,
                                              heart_rate = hr,
                                              player_load = pl,
                                              smoothed_load = sl,
                                              metabolic_power = mp,
                                              positional_quality = pq,
                                              number_satelites = ref))


```

To make it easier going forward I wrap it all in a function:

```{r get data function,  echo=TRUE, eval=FALSE}

get_raw_data <- function(name, startdate, enddate, stoken, sregion) {

#Add the variables to extract the data of interest
playerName <- name
startDate <- startdate
endDate <- enddate

#Add the variables for the API connection
sToken <- stoken
sRegion <- sregion

token <- ofCloudCreateToken(sToken = sToken,  sRegion = sRegion)

#And just copy-paste the above code chunks in here

...

}
```

The final step I need to solve before I can use the function is to identify the __sToken__ and the __sRegion__ (These are text strings used to access your user profile via the API, and can can be found in the Catapult Cloud). With that in place, I am ready to collect some raw 10 Hz data by simply adding the parameters to the function:

```{r get data example,  echo=TRUE, eval=FALSE}

#I want to get data from Player A in date range 5th to 10th January 2024
data <- get_raw_data(name = "Player A", 
                            startdate = "05-01-2024", 
                            enddate = "10-01-2024", 
                            stoken = "xxx", 
                            sregion = "xxx")

```

The data comes in a list format, with each list representing an activity. For this example, I get 4 activities (training sessions only), which is on the low end for obtaining a reliable AS profile, but that is not the main focus for now.

```{r load the data, echo=FALSE}

#Preview of the data
load("data.RData")

```

The raw data looks like this with each row representing 0.1 seconds (sampling rate of 10 Hz) (only a few columns are selected):

```{r preview data, echo=FALSE, message=FALSE}

#Preview of the data (selecting a few variables)
data[[1]] %>% 
  select(athlete_first_name, velocity, velocity_raw, acceleration, seconds, elapsed_time) %>%
  top_n(10) %>%
  kableExtra::kable()


```

#### Preparation of the raw data

For the preparation of the speed acceleration data, I use the __Signal__ package and a butterworth filter as outlined in __[Clavel et al., 2023](https://www.sciencedirect.com/science/article/abs/pii/S0021929023001719)__.

I use the filter on the raw velocity data, as opposed to the velocity data already filtered by Catapult, to secure consistency. 
```{r butterworth, echo=TRUE, message=FALSE}

library(signal)

#Set filter details
bf4 <- butter(2, 0.10, type="low")

#Apply filter
rawData_list <- data %>%
                    #Delete NA's
                    map(~na.omit(.)) %>%
                    #Apply filter on raw velocity data across lists
                    map(~mutate(.data = ., velocity_raw_bw = filtfilt(bf4, .$velocity_raw)))

```

I then calculate instantaneous acceleration based on the filtered velocity data. There are a few ways this can be done. I use a running linear regression with a window of 3. This means that three consecutive velocity observations are used to create a linear regression with the slope of the being the acceleration. It makes the calculation of acceleration a little less sensitive to outliers in velocity data.

```{r acceleration, echo=TRUE, message=FALSE}
 library(runner)
 library(useful)


 rawData_list_acc <- rawData_list %>%
                     map(~mutate(.data = .,
                                 reg_acc = runner(x = .,
                                                  k = 3,
                                                  na_pad = FALSE,
                                                  f = function(x){
                                                  model <- lm(velocity_raw_bw ~ elapsed_time,
                                                  data = x)
                                   coefficients(model)[2]})))

 # I shift the calculated acceleration column 1 row up since the first value is NA
 rawData_list_shifted <- rawData_list_acc %>%
                                      map(~shift.column(data = .,
                                                        columns = "reg_acc",
                                                        newNames = "acc",
                                                        up = TRUE, len = 1))

 # Finally, I bind the lists into a single data frame
 all_data <- rawData_list_shifted %>%
                             bind_rows()

```

#### Inspection of the data

To quickly check that the velocity and acceleration traces make sense, I visualize a snapshot of the traces. It seems to fit (acceleration is the red line).

```{r velocity trace, echo=FALSE}

library(ggpubr)

trial_mutate_slice <- all_data %>% slice(25570:25700)

trial_mutate_slice <- trial_mutate_slice %>% mutate(time = row_number())

ggplot(trial_mutate_slice, aes (x = elapsed_time)) +
  geom_line(aes(y= velocity_raw_bw), color = "black", linewidth = 1) +
  geom_line(aes(y = acc), color = "red", linewidth = 1) +
  theme_pubclean() +
  xlab("Sample") +
  ylab("Velocity (m/s)")

```

The second inspection is the scatter plot with acceleration on the y-axis and velocity on the x-axis. This depicts the entire spectrum of instantaneous acceleration-velocity and gives you a good impression on the data quality. E.g. if you see high accelerations towards high velocities you should be concerned. Also the maximum acceleration should typically be found at a velocity of less then 3 m/s.

```{r scatterplot, echo=TRUE, warning=FALSE}


 all_data %>%
  ggplot(aes(x = velocity_raw_bw, y = acc)) +
        geom_point(size = 0.2) +
        ylim(0,NA) +
        geom_vline(xintercept = 3, color = "red") +
        theme_pubclean() +
        xlab("Velocity (m/s)") +
        ylab("Acceleration (m/s-2)")

```

The data looks good, and I proceed to the final step. I use the __[InSituASProfile](https://lasseishoi.github.io/InSituASProfile/index.html)__ package, which I have developed. I first prepare the data using the __prepare_data__ function, which also creates an initial regression line, so that one can decide if it is feasible to proceed to the final calculation. In this case, points are located very close to the regression line with an r-square value of 0.98 and I proceed to the final model model/calculation.

```{r AS profile, echo=TRUE, warning=FALSE, message=FALSE}


library(InSituASProfile)

# I rename the column as the InSituASProfile package need a "acc" and "speed" column
 all_data <- all_data %>%
            rename(speed = velocity_raw_bw)
 
 
# Prepare data and inition plot
InSituASProfile::prepare_data(all_data, print_plot = TRUE)

```

```{r final model, warning=FALSE, message=FALSE}

InSituASProfile::get_AS_Profile()


```

